{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Завдання 1: Виклик LLM з базовим промптом\n",
    "\n",
    "Створіть можливість викликати LLM зі звичайним текстовим промптом.\n",
    "\n",
    "Промпт має дозвляти отримати інформацію простою мовою на певну тему. В цьому завданні ми хочемо дізнатись про тему \"Квантові обчислення\".\n",
    "\n",
    "Відповідь моделі повинна містити визначення, ключові переваги та поточні дослідження в цій галузі.\n",
    "\n",
    "Обмежте відповідь до 200 символів і пропишіть в промпті аби відповідь була короткою (це зекономить Вам час і гроші на згенеровані токени).\n",
    "\n",
    "В якості LLM можна скористатись як моделлю з HugginFace (рекомендую Mistral), так і ChatGPT4 або ChatGPT3. В обох випадках треба імпортувати потрібну \"обгортку\" (тобто клас, який дозволить ініціювати модель) з LangChain для виклику LLM за API, а також зчитати особистий токен з файла, наприклад, `creds.json`, який розміщений у Вас локально і Ви НЕ здаєте його в ДЗ і НЕ комітите в git 😏\n",
    "\n",
    "Встановіть своє значення температури на свій розсуд (тут немає правильного чи неправильного значення) і напишіть, чому ви обрали саме таке значення для цього завдання.  \n",
    "\n",
    "Запити можна робити як українською, так і англійською - орієнтуйтесь на те, де і чи хочете ви потім лишити цей проєкт і відповідна яка мова буде пасувати більше. В розвʼязках промпти - українською."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:56:25.599177Z",
     "start_time": "2025-04-06T14:56:25.596420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_tavily import TavilySearch"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# Read credentials from a local file\n",
    "with open('../creds.json') as file:\n",
    "  creds = json.load(file)\n",
    "\n",
    "# Prepare the environment\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = creds['HUGGINGFACEHUB_API_TOKEN']\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"SERPAPI_API_KEY\"] = creds[\"SERPAPI_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = creds[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"OPENWEATHERMAP_API_KEY\"] = creds['OPENWEATHERMAP_API_KEY']"
   ],
   "metadata": {
    "id": "vT1ICe74cAUS",
    "ExecuteTime": {
     "end_time": "2025-04-06T15:03:12.298039Z",
     "start_time": "2025-04-06T15:03:12.295615Z"
    }
   },
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:39:48.384800Z",
     "start_time": "2025-04-06T13:39:47.903420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set overall temperature. Set it between creativity and accuracy.\n",
    "# Since the task is not about creative writing so it should not be more than ~0.5-0.6\n",
    "overall_temperature = 0.2\n",
    "\n",
    "mistral = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    temperature=overall_temperature,\n",
    "    max_new_tokens=512,  # in general 1 English word = 1.3 tokens or 0.75 work costs 1 token :)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:39:48.478464Z",
     "start_time": "2025-04-06T13:39:48.476767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "topic = \"Квантові обчислення\"\n",
    "\n",
    "prompt = (\n",
    "f\"\"\"\n",
    "Ти розумний асистент, який пояснює складні теми простими словами. Твоя задача в максимально коротку відповідь надати визначення, ключові переваги та поточні дослідження в галузі {topic}.\n",
    "\n",
    "Відповідь має бути короткою, не більше 200 символів.\n",
    "Відповідаю тією мовою, якою запитують.\n",
    "\"\"\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:54:38.307392Z",
     "start_time": "2025-04-06T13:54:37.683775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Call the model\n",
    "# Invoke the model\n",
    "response = mistral.invoke(prompt)\n",
    "\n",
    "# Output the result\n",
    "print(response)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytro/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-67f2879e-64c453921226729d0ceadfe8;bfd9ba29-498e-405f-be0f-cdfc01725ff2)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 409\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Call the model\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Invoke the model\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mmistral\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Output the result\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    387\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    388\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    402\u001B[0m     )\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:763\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    757\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    760\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    761\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    762\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:966\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    952\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    953\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    954\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    964\u001B[0m         )\n\u001B[1;32m    965\u001B[0m     ]\n\u001B[0;32m--> 966\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:787\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    778\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    779\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    786\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 787\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    791\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    794\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    795\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    796\u001B[0m         )\n\u001B[1;32m    797\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    798\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1526\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1523\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1525\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1526\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1527\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1528\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m     )\n\u001B[1;32m   1530\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001B[0m, in \u001B[0;36mHuggingFaceEndpoint._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    309\u001B[0m     invocation_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m invocation_params[\n\u001B[1;32m    310\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    311\u001B[0m     ]  \u001B[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minvocation_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     response_text \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001B[39;00m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001B[39;00m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:132\u001B[0m, in \u001B[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m     warning_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m message\n\u001B[1;32m    131\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(warning_message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:268\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    266\u001B[0m url \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39m_prepare_url(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken, mapped_model)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    267\u001B[0m headers \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39m_prepare_headers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRequestParameters\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munknown\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munknown\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:321\u001B[0m, in \u001B[0;36mInferenceClient._inner_post\u001B[0;34m(self, request_parameters, stream)\u001B[0m\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:481\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 481\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-67f2879e-64c453921226729d0ceadfe8;bfd9ba29-498e-405f-be0f-cdfc01725ff2)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Нотатки для куратора\n",
    "\n",
    "До цього все нормально запускалось, видимо, що там вийшов за ліміти."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Завдання 2: Створення параметризованого промпта для генерації тексту\n",
    "Тепер ми хочемо оновити попередній фукнціонал так, аби в промпт ми могли передавати тему як параметр. Для цього скористайтесь `PromptTemplate` з `langchain` і реалізуйте параметризований промпт та виклик моделі з ним.\n",
    "\n",
    "Запустіть оновлений функціонал (промпт + модел) для пояснень про теми\n",
    "- \"Баєсівські методи в машинному навчанні\"\n",
    "- \"Трансформери в машинному навчанні\"\n",
    "- \"Explainable AI\"\n",
    "\n",
    "Виведіть результати відпрацювання моделі на екран."
   ],
   "metadata": {
    "id": "UiIzV0UIS0GA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "template = (\n",
    "\"\"\"Ти розумний асистент, який пояснює складні теми простими словами. Твоя задача в максимально коротку відповідь надати визначення, ключові переваги та поточні дослідження в галузі {topic}.\n",
    "\n",
    "Відповідь має бути короткою, не більше 200 символів.\n",
    "Відповідаю тією мовою, якою запитують.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ],
   "metadata": {
    "id": "kcg7EfdNcBDq",
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:01.626273Z",
     "start_time": "2025-04-06T13:56:01.623961Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:03.415366Z",
     "start_time": "2025-04-06T13:56:02.918381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Call the model\n",
    "# Invoke the model\n",
    "llm_chain = prompt | mistral\n",
    "\n",
    "topics = [\"Баєсівські методи в машинному навчанні\", \"Трансформери в машинному навчанні\", \"Explainable AI\"]\n",
    "\n",
    "for topic in topics:\n",
    "  print(llm_chain.invoke({'topic': topic}))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytro/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-67f287f3-574f0e963297a30c4af4315e;b8fe663c-cdd1-4770-8c42-8f4df0d0040f)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 409\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m topics \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mБаєсівські методи в машинному навчанні\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mТрансформери в машинному навчанні\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExplainable AI\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m topic \u001B[38;5;129;01min\u001B[39;00m topics:\n\u001B[0;32m----> 8\u001B[0m   \u001B[38;5;28mprint\u001B[39m(\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtopic\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopic\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/runnables/base.py:3025\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3023\u001B[0m                 \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(step\u001B[38;5;241m.\u001B[39minvoke, \u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   3024\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3025\u001B[0m                 \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3026\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3027\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    381\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    387\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    388\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 390\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    401\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    402\u001B[0m     )\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:763\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    757\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    760\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    761\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    762\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 763\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:966\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    952\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    953\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    954\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    964\u001B[0m         )\n\u001B[1;32m    965\u001B[0m     ]\n\u001B[0;32m--> 966\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    969\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:787\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    777\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    778\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    779\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    785\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    786\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 787\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    791\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    794\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    795\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    796\u001B[0m         )\n\u001B[1;32m    797\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    798\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1526\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1523\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1525\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1526\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1527\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1528\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1529\u001B[0m     )\n\u001B[1;32m   1530\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001B[0m, in \u001B[0;36mHuggingFaceEndpoint._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    309\u001B[0m     invocation_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m invocation_params[\n\u001B[1;32m    310\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    311\u001B[0m     ]  \u001B[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001B[39;00m\n\u001B[0;32m--> 312\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minvocation_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     response_text \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001B[39;00m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001B[39;00m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:132\u001B[0m, in \u001B[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    130\u001B[0m     warning_message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m message\n\u001B[1;32m    131\u001B[0m warnings\u001B[38;5;241m.\u001B[39mwarn(warning_message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 132\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:268\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    266\u001B[0m url \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39m_prepare_url(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken, mapped_model)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    267\u001B[0m headers \u001B[38;5;241m=\u001B[39m provider_helper\u001B[38;5;241m.\u001B[39m_prepare_headers(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheaders, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRequestParameters\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munknown\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43munknown\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    276\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:321\u001B[0m, in \u001B[0;36mInferenceClient._inner_post\u001B[0;34m(self, request_parameters, stream)\u001B[0m\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:481\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    479\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 481\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.3 (Request ID: Root=1-67f287f3-574f0e963297a30c4af4315e;b8fe663c-cdd1-4770-8c42-8f4df0d0040f)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Завдання 3: Використання агента для автоматизації процесів\n",
    "Створіть агента, який допоможе автоматично шукати інформацію про останні наукові публікації в різних галузях. Наприклад, агент має знайти 5 останніх публікацій на тему штучного інтелекту.\n",
    "\n",
    "**Кроки:**\n",
    "1. Налаштуйте агента типу ReAct в LangChain для виконання автоматичних запитів.\n",
    "2. Створіть промпт, який спрямовує агента шукати інформацію в інтернеті або в базах даних наукових публікацій.\n",
    "3. Агент повинен видати список публікацій, кожна з яких містить назву, авторів і короткий опис.\n",
    "\n",
    "Для взаємодії з пошуком там необхідно створити `Tool`. В лекції ми використовували `serpapi`. Можна продовжити користуватись ним, або обрати інше АРІ для пошуку (вони в тому числі є безкоштовні). Перелік різних АРІ, доступних в langchain, і орієнтир по вартості запитів можна знайти в окремому документі [тут](https://hannapylieva.notion.site/API-12994835849480a69b2adf2b8441cbb3?pvs=4).\n",
    "\n",
    "Лишаю також нижче приклад використання одного з безкоштовних пошукових АРІ - DuckDuckGo (не потребує створення токена!)  - можливо він вам сподобається :)\n"
   ],
   "metadata": {
    "id": "m9UsL2gXSe-3"
   }
  },
  {
   "cell_type": "code",
   "source": "#!pip install -q langchain_community duckduckgo_search",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jzj4PHP9XwH-",
    "outputId": "08118bad-e4c6-4873-feb0-e3890dc5c6a4",
    "ExecuteTime": {
     "end_time": "2025-04-06T13:39:49.175418Z",
     "start_time": "2025-04-03T16:21:35.216103Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "search.invoke(\"Obama's first name?\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "p56AZ_SnXvTs",
    "outputId": "fcee82cc-03d7-4402-a243-0033596a9ea9",
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:09.982701Z",
     "start_time": "2025-04-06T13:56:08.821009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The White House, official residence of the president of the United States, in 2008. The president of the United States is the head of state and head of government of the United States, [1] indirectly elected to a four-year term via the Electoral College. [2] Under the U.S. Constitution, the officeholder leads the executive branch of the federal government and is the commander-in-chief of the ... Obama's father, Barack Obama, Sr., was a teenage goatherd in rural Kenya, won a scholarship to study in the United States, and eventually became a senior economist in the Kenyan government.Obama's mother, S. Ann Dunham, grew up in Kansas, Texas, and Washington state before her family settled in Honolulu.In 1960 she and Barack Sr. met in a Russian language class at the University of Hawaii ... Since 1789 and the election of George Washington, America's first president, 45 individuals have served as the chief executive of the United States ... Barack Obama: Joseph Biden: Democratic: 2009-2017: Donald Trump: Mike Pence: Republican: 2017-2021: Joseph Biden: Kamala Harris: Democratic: 2021- Jimmy, the first president to use his nickname in an official capacity, rather than his first name James. [195] Jimmy Cardigan, got the nickname after he wore a sweater instead of a suit one day [18] The Peanut Farmer, [196] he owned a peanut farm and fostered this image in his early campaigns, as a contrast to elite Washington insiders. The Irish Sun, It's a fake Barack Obama's brother posts forged document he claims is ex-president's 'Kenyan birth certificate,' March 11, 2017 Salon, Orly Taitz is at it again , Sept. 4, 2009\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:12.089437Z",
     "start_time": "2025-04-06T13:56:11.761505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=overall_temperature, max_tokens=512)\n",
    "\n",
    "tools = load_tools([\"serpapi\"], llm=llm)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "react_prompt = prompt + \"Always follow the ReAct format strictly. You're an extraction specialist.\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytro/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:14.719439Z",
     "start_time": "2025-04-06T13:56:14.714825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tavily_tool = TavilySearch(\n",
    "    max_results=1,\n",
    "    topic=\"news\",\n",
    "    include_answer=True\n",
    ")\n",
    "\n",
    "tools = [tavily_tool]\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=6, )"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:56:56.377663Z",
     "start_time": "2025-04-06T13:56:16.611585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query = \"\"\"Your task is to search for and identify the 5 most recent publications on this topic. For each publication, you need to provide the following information:\n",
    "# 1. Title\n",
    "# 2. Authors\n",
    "# 3. A short description (1-2 sentences summarizing the main points of the publication)\n",
    "# \"\"\"\n",
    "\n",
    "query = \"\"\"Here is the topic you need to search for - Artificial Intelligence.\n",
    "\n",
    "Your task is to search for and identify the 5 most recent publications on this topic. For each publication, you need to provide the following information:\n",
    "1. Title\n",
    "2. Authors\n",
    "3. A short description (1-2 sentences summarizing the main points of the publication)\n",
    "\n",
    "Please follow these steps:\n",
    "\n",
    "1. Search for publications related to the given topic.\n",
    "2. Identify the 5 most recent publications.\n",
    "3. For each publication, extract the required information (title, authors, short description).\n",
    "4. Format the results in a numbered list for easy readability.\n",
    "\n",
    "Here's an example of how your final output should be formatted:\n",
    "\n",
    "1. [Title of Publication 1]\n",
    "   Authors: [Author 1], [Author 2], et al.\n",
    "   Description: [1-2 sentence summary of the publication]\n",
    "\n",
    "2. [Title of Publication 2]\n",
    "   Authors: [Author 1], [Author 2], et al.\n",
    "   Description: [1-2 sentence summary of the publication]\n",
    "\n",
    "(Continue for all 5 publications)\n",
    "\n",
    "Please ensure that your list is clear, concise, and easy to read. Remember to focus on the most recent publications and provide accurate information for each entry.\"\"\"\n",
    "\n",
    "agent_executor.invoke({'input': query })"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI need to search for the most recent publications on Artificial Intelligence to gather the required information. \n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"recent publications on Artificial Intelligence 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'recent publications on Artificial Intelligence 2023', 'follow_up_questions': None, 'answer': 'Recent advancements in artificial intelligence were published in 2023, featuring research from the ICRAAI conference in Jaipur, India. The book includes papers presented at the event held in December 2023. This publication remains relevant for current AI developments.', 'images': [], 'results': [{'title': 'Recent Advancements in Artificial Intelligence - Springer', 'url': 'https://link.springer.com/book/10.1007/978-981-97-1111-6', 'content': 'Presents recent research in artificial intelligence; Discusses the outcomes of ICRAAI 2023, held in Jaipur, India ... This book features research papers presented at the Second International Conference on Recent Advancements in Artificial Intelligence (ICRAAI 2023), held at Poornima University, Jaipur, India during 15 - 16 December 2023', 'score': 0.8846886, 'raw_content': None}], 'response_time': 1.77}\u001B[0m\u001B[32;1m\u001B[1;3mI need to refine my search to find more recent publications on Artificial Intelligence, as I only found one publication so far.\n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"latest research papers on Artificial Intelligence 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'latest research papers on Artificial Intelligence 2023', 'follow_up_questions': None, 'answer': 'In 2023, notable AI research included Pythia, Llama 2, and Zephyr 7B models. Mistral 7B and DPO methods gained attention. Efficient finetuning techniques like QLoRA were also significant.', 'images': [], 'results': [{'url': 'https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023', 'title': 'Ten Noteworthy AI Research Papers of 2023 - Ahead of AI', 'content': \"To conclude an eventful 2023 in machine learning and AI research, I'm excited to share 10 noteworthy papers I've read this year. My personal focus has been more on large language models, so you'll find a heavier emphasis on large language model (LLM) papers than computer vision papers this year. The selection criteria were based on a mix of papers I either particularly enjoyed or found impactful and worth noting. Llama 2 models, which range from 7B to 70B parameters, are one of the reasons this paper made it onto this list: these are still among the most capable and widely used openly available models. I include it because it's an interesting case study where someone pretrained a relatively large LLM on a domain-specific dataset.\", 'score': 0.8936907, 'raw_content': None}], 'response_time': 12.1}\u001B[0m\u001B[32;1m\u001B[1;3mI still need to find more recent publications on Artificial Intelligence, as I have only identified two publications so far.\n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"latest AI research publications 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'latest AI research publications 2023', 'follow_up_questions': None, 'answer': 'In 2023, notable AI research papers included Sparks of AGI by Microsoft and DreamerV3 by DeepMind. These papers advanced language models and reinforcement learning. Key areas were AGI, multimodal models, and generative agents.', 'images': [], 'results': [{'title': 'Top 10 Influential AI Research Papers in 2023 from Google ... - TOPBOTS', 'url': 'https://www.topbots.com/top-ai-research-papers-2023/', 'content': 'Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More Skip to main content Skip to primary sidebar Skip to footer Menu Item Topics Computer Vision Conversational AI Infrastructure Natural Language Processing HR & Recruiting Marketing Research Summaries Technical Guides Applied AI AI Research Login TOPBOTS The Best of Applied Artificial Intelligence, Machine Learning, Automation, Bots, Chatbots Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More December 5, 2023 by Mariya Yao From Generative Agents research paper In this article, we delve into ten transformative research papers from diverse domains, spanning language models, image processing, image generation, and video editing. As discussions around Artificial General Intelligence (AGI) reveal that AGI seems more approachable than ever, it’s no wonder that some of the featured papers explore various paths to AGI, such as extending language models or harnessing reinforcement learning for domain-spanning mastery. The team argues that GPT-4 represents a new class of large language models, exhibiting more generalized intelligence compared to previous AI models. From innovative language models to groundbreaking image generation and video editing techniques, these papers have pushed the boundaries of AI capabilities.', 'score': 0.90593183, 'raw_content': None}], 'response_time': 2.03}\u001B[0m\u001B[32;1m\u001B[1;3mI still need to find more recent publications on Artificial Intelligence, as I have only identified three publications so far.\n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"recent AI research papers December 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'recent AI research papers December 2023', 'follow_up_questions': None, 'answer': 'Recent AI research includes Sparks of AGI by Microsoft, PALM-E by Google, and LLaMA 2 by Meta AI. These papers focus on language models, multimodal integration, and large language model enhancements.', 'images': [], 'results': [{'title': 'Top 10 Influential AI Research Papers in 2023 from Google ... - TOPBOTS', 'url': 'https://www.topbots.com/top-ai-research-papers-2023/', 'content': 'Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More Skip to main content Skip to primary sidebar Skip to footer Menu Item Topics Computer Vision Conversational AI Infrastructure Natural Language Processing HR & Recruiting Marketing Research Summaries Technical Guides Applied AI AI Research Login TOPBOTS The Best of Applied Artificial Intelligence, Machine Learning, Automation, Bots, Chatbots Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More December 5, 2023 by Mariya Yao From Generative Agents research paper In this article, we delve into ten transformative research papers from diverse domains, spanning language models, image processing, image generation, and video editing. As discussions around Artificial General Intelligence (AGI) reveal that AGI seems more approachable than ever, it’s no wonder that some of the featured papers explore various paths to AGI, such as extending language models or harnessing reinforcement learning for domain-spanning mastery. The team argues that GPT-4 represents a new class of large language models, exhibiting more generalized intelligence compared to previous AI models. From innovative language models to groundbreaking image generation and video editing techniques, these papers have pushed the boundaries of AI capabilities.', 'score': 0.9332426, 'raw_content': None}], 'response_time': 4.77}\u001B[0m\u001B[32;1m\u001B[1;3mI still need to find more recent publications on Artificial Intelligence, as I have only identified three publications so far.\n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"latest AI research publications December 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'latest AI research publications December 2023', 'follow_up_questions': None, 'answer': 'In 2023, Nature Machine Intelligence published articles on particle tracking, neural network generalization, and AI-powered electronic skin. Key themes included advancements in AI safety, machine learning for drug design, and the integration of AI with neuroscience.', 'images': [], 'results': [{'title': 'Articles in 2023 | Nature Machine Intelligence', 'url': 'https://www.nature.com/natmachintell/articles?year=2023', 'content': 'A graph neural network-enhanced method enables accurate particle tracking by significantly reducing the number of lost trajectories. The framework provides a way to interpret the effects of regularization schemes such as stochastic gradient descent and dropout on generalization — and to improve upon these methods. Liu and colleagues trained MoleculeSTM, a foundation model that aligns the structure and text modalities through contrastive learning, and show its utility on the downstream tasks of structure–text retrieval, text-guided editing and molecular property prediction. review the integration of AI methods and electronic skins, especially how data collected from sensors are processed by AI to extract features for human–machine interactions and health monitoring purposes.', 'score': 0.6609831, 'raw_content': None}], 'response_time': 4.15}\u001B[0m\u001B[32;1m\u001B[1;3mI still need to find more recent publications on Artificial Intelligence, as I have only identified three publications so far.\n",
      "\n",
      "Action: tavily_search  \n",
      "Action Input: \"AI research papers published in December 2023\"  \u001B[0m\u001B[36;1m\u001B[1;3m{'query': 'AI research papers published in December 2023', 'follow_up_questions': None, 'answer': 'In December 2023, notable AI research papers included Sparks of AGI by Microsoft and PALM-E by Google. These papers focused on advancements in language models and multimodal language models.', 'images': [], 'results': [{'title': 'Top 10 Influential AI Research Papers in 2023 from Google ... - TOPBOTS', 'url': 'https://www.topbots.com/top-ai-research-papers-2023/', 'content': 'Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More Skip to main content Skip to primary sidebar Skip to footer Menu Item Topics Computer Vision Conversational AI Infrastructure Natural Language Processing HR & Recruiting Marketing Research Summaries Technical Guides Applied AI AI Research Login TOPBOTS The Best of Applied Artificial Intelligence, Machine Learning, Automation, Bots, Chatbots Top 10 Influential AI Research Papers in 2023 from Google, Meta, Microsoft, and More December 5, 2023 by Mariya Yao From Generative Agents research paper In this article, we delve into ten transformative research papers from diverse domains, spanning language models, image processing, image generation, and video editing. As discussions around Artificial General Intelligence (AGI) reveal that AGI seems more approachable than ever, it’s no wonder that some of the featured papers explore various paths to AGI, such as extending language models or harnessing reinforcement learning for domain-spanning mastery. The team argues that GPT-4 represents a new class of large language models, exhibiting more generalized intelligence compared to previous AI models. From innovative language models to groundbreaking image generation and video editing techniques, these papers have pushed the boundaries of AI capabilities.', 'score': 0.9529303, 'raw_content': None}], 'response_time': 2.7}\u001B[0m\u001B[32;1m\u001B[1;3m\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"Here is the topic you need to search for - Artificial Intelligence.\\n\\nYour task is to search for and identify the 5 most recent publications on this topic. For each publication, you need to provide the following information:\\n1. Title\\n2. Authors\\n3. A short description (1-2 sentences summarizing the main points of the publication)\\n\\nPlease follow these steps:\\n\\n1. Search for publications related to the given topic.\\n2. Identify the 5 most recent publications.\\n3. For each publication, extract the required information (title, authors, short description).\\n4. Format the results in a numbered list for easy readability.\\n\\nHere's an example of how your final output should be formatted:\\n\\n1. [Title of Publication 1]\\n   Authors: [Author 1], [Author 2], et al.\\n   Description: [1-2 sentence summary of the publication]\\n\\n2. [Title of Publication 2]\\n   Authors: [Author 1], [Author 2], et al.\\n   Description: [1-2 sentence summary of the publication]\\n\\n(Continue for all 5 publications)\\n\\nPlease ensure that your list is clear, concise, and easy to read. Remember to focus on the most recent publications and provide accurate information for each entry.\",\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### Завдання 4: Створення агента-помічника для вирішення бізнес-задач\n",
    "\n",
    "Створіть агента, який допомагає вирішувати задачі бізнес-аналітики. Агент має допомогти користувачу створити прогноз по продажам на наступний рік враховуючи рівень інфляції і погодні умови. Агент має вміти використовувати Python і ходити в інтернет аби отримати актуальні дані.\n",
    "\n",
    "**Кроки:**\n",
    "1. Налаштуйте агента, який працюватиме з аналітичними даними, заданими текстом. Користувач пише\n",
    "\n",
    "```\n",
    "Ми експортуємо апельсини з Бразилії. В 2021 експортували 200т, в 2022 - 190т, в 2023 - 210т, в 2024 який ще не закінчився - 220т. Зроби оцінку скільки ми зможемо експортувати апельсинів в 2025 враховуючи погодні умови в Бразилії і попит на апельсини в світі виходячи з економічної ситуації.\n",
    "```\n",
    "\n",
    "2. Створіть запит до агента, що містить чітке завдання – видати результат бізнес аналізу або написати, що він не може цього зробити і запит користувача (просто може бути все одним повідомлленням).\n",
    "\n",
    "3. Запустіть агента і проаналізуйте результати. Що можна покращити?\n"
   ],
   "metadata": {
    "id": "IOqujC6qY_NY"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UK6KA9PyRS0L",
    "ExecuteTime": {
     "end_time": "2025-04-06T14:57:23.947220Z",
     "start_time": "2025-04-06T14:57:23.933259Z"
    }
   },
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=512)\n",
    "\n",
    "# Prepare the tools\n",
    "python_repl = PythonREPL()\n",
    "python_interpreter_tool = Tool(\n",
    "    name=\"python_interpreter\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you necessarily should print it out with `print(...)`. Otherwise you won't see the result! It's very important.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "problem_chain = LLMMathChain.from_llm(llm=llm)\n",
    "math_tool = Tool.from_function(name=\"Calculator\",\n",
    "                func=problem_chain.run,\n",
    "                description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions.\")\n",
    "\n",
    "tools = load_tools(['serpapi', 'openweathermap-api'], llm=llm)\n",
    "\n",
    "tools.append(python_interpreter_tool)\n",
    "tools.append(math_tool)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:05:55.868694Z",
     "start_time": "2025-04-06T15:05:55.625675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = hub.pull('hwchase17/react')\n",
    "react_prompt = prompt + \"Always follow the ReAct format strictly. You're an extraction specialist.\"\n",
    "\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, max_iterations=10)\n",
    "\n",
    "query = \"\"\"\n",
    "Ми експортуємо апельсини з Бразилії. В 2021 — 200т, в 2022 — 190т, в 2023 — 210т, в 2024 (ще не завершено) — 220т.\n",
    "Зроби оцінку на 2025 рік, враховуючи:\n",
    "- погодні умови в Бразилії (використовуй openweathermap tool)\n",
    "- попит на апельсини у світі (використовуй SerpAPI tool),\n",
    "- економічну ситуацію (використовуй SerpAPI tool).\n",
    "\n",
    "Видай мені наступний результат:\n",
    "1. Знайди останні новини про погодні умови в Бразилії.\n",
    "2. Знайди статті про глобальний попит на апельсини.\n",
    "3. Оціни тренд експорту.\n",
    "4. Зроби прогноз на 2025 (в тоннах) з поясненням.\n",
    "\n",
    "Якщо не можеш знайти інформацію, то напиши, що не можеш знайти дані і не пробуй прогнозувати. Треба лише чітка оцінка.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmytro/Code/MachineLearning/machine-learning-for-humans-2.0/.env/lib/python3.12/site-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:06:25.902603Z",
     "start_time": "2025-04-06T15:06:06.120958Z"
    }
   },
   "cell_type": "code",
   "source": "agent_executor.invoke({'input': query})",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI need to gather information on the weather conditions in Brazil, global demand for oranges, and the economic situation to make an informed estimate for orange exports in 2025. \n",
      "\n",
      "Action: Search\n",
      "Action Input: \"latest weather news Brazil October 2023\"\n",
      "\n",
      "Thought: I will first find the latest weather news in Brazil to understand the current conditions affecting orange production. \n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m[' entity_type: related_questions.', 'As per the latest update from Fundecitrus, there is a decline in the production of oranges for the São Paulo and West-Southwest Minas Gerais citrus belt in the ...', 'Fundecitrus and its cooperators released an orange crop forecast on Feb. 10. Total production is forecast to be 228.52 million boxes for 2024–25.', 'This update highlights the resilience of the Brazilian orange industry amid climatic challenges, with production forecasts remaining robust.', \"Brazil's orange juice production will drop by almost 30% compared to the previous season, according to the latest projections by Fundecitrus.\", 'The Brazilian orange crop for Marketing Year (MY) 2024/25 is forecast at 320 million 90-pound boxes (MBx) - standard reference, equivalent to 13 million metric ...', \"The forecast for the season is 232.38 million boxes of 40.8 kg each, representing a 24.36% decrease compared to the previous season's 307.22 million boxes.\", \"It is the world's fifth-largest country by area and the seventh largest by population, with over 212 million people.\", \"This year's GRPS has brought together leading insights on the evolving global risks landscape from 1,490 experts across academia, business, ...\", 'Fundecitrus now estimates 223.14 million 40.8-kg boxes, which is promising news after recent declines due to severe weather and widespread greening disease.', 'Climate change is affecting food security in drylands, particularly those in Africa, and high mountain regions of Asia and South America.']\u001B[0m\u001B[32;1m\u001B[1;3mI have found some relevant information regarding the weather conditions affecting orange production in Brazil. The latest updates indicate a decline in orange production in the São Paulo and West-Southwest Minas Gerais regions, with forecasts suggesting a decrease in total production for the upcoming season. However, there are also indications of resilience in the Brazilian orange industry despite climatic challenges.\n",
      "\n",
      "Next, I need to gather information on global demand for oranges.\n",
      "\n",
      "Action: Search  \n",
      "Action Input: \"global demand for oranges 2023 news\"\n",
      "\n",
      "Thought: I will look for articles that discuss the current global demand for oranges to understand market trends. \n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m[' entity_type: related_questions.', 'The global orange market size was estimated at 46969 kilo tons in 2023 and is projected to grow at a CAGR of 3.2% from 2024 to 2030.', 'Major trends include rising demand for organic and sustainably sourced oranges, the development of new varieties with enhanced shelf life, ...', 'Our report on the orange market provides a holistic analysis, market size and forecast, trends, growth drivers, and challenges, as well as ...', 'The orange market size, currently valued at $3.44 billion, is projected to grow to $3.64 billion in 2024, at a compound annual growth rate CAGR ...', 'The global Oranges market is projected to experience significant growth between 2024 and 2033. This momentum is driven by the steady growth ...', 'The global oranges market size reached 48.82 Million Tons in 2023. By 2032, IMARC Group expects the market to reach 59.9 Million Tons, at a projected CAGR of ...', 'Fresh Trends 2023 reveals that consumers with kids at home were more likely to buy oranges, with 51% reporting purchases in the past year, ...', 'Oranges Market size will be USD 3915.2 million in 2024 and will expand at a compound annual growth rate (CAGR) of 4.80% from 2024 to 2031.', 'This paper provides an overview of the varietal panorama together with a description of the main driving forces in present and future sweet orange breeding.', \"America throws away nearly 60 million tons of food every year. That's almost 40 percent of the entire US food supply. Learn why and how you can help.\"]\u001B[0m\u001B[32;1m\u001B[1;3mI have found relevant information regarding the global demand for oranges. The global orange market size was estimated at approximately 46,969 kilotons in 2023 and is projected to grow at a compound annual growth rate (CAGR) of 3.2% from 2024 to 2030. This indicates a steady increase in demand, driven by trends such as rising interest in organic and sustainably sourced oranges.\n",
      "\n",
      "Next, I need to gather information on the economic situation that could affect orange exports.\n",
      "\n",
      "Action: Search  \n",
      "Action Input: \"current economic situation Brazil October 2023\"\n",
      "\n",
      "Thought: I will look for articles that discuss the current economic situation in Brazil to understand how it might impact orange exports. \n",
      "\u001B[0m\u001B[36;1m\u001B[1;3m[' entity_type: related_questions.', 'The economy has done well in 2023, with strong wages and exports, but restraining inflation poses a challenge ahead. Explore the dynamics at play.', \"Brazil's economic growth has shown resilience, averaging above 3% over the past three years. Strong private consumption, driven by social ...\", 'Domestic demand drove robust 2024 growth in Brazil, but inflation and a weak currency can hinder further progress this year.', \"Brazil is the world's leading orange grower and processes most of its production into a range of orange products driven by orange juice concentrate. This ...\", 'As per the latest update from Fundecitrus, there is a decline in the production of oranges for the São Paulo and West-Southwest Minas Gerais citrus belt in the ...', 'The global orange juice market produces 1.4 million tons and earns USD 7.5 billion annually. The current production decline, however, impacts ...', \"Brazil's economy has recovered steam post-COVID-19, fuelled by exports and household consumption. However, it faces a large public debt and policy uncertainty, ...\", \"The majority of deforestation has occurred in Brazil, followed by Bolivia, Colombia, and Peru. The widespread loss of forest jeopardizes the Amazon's ability to ...\", 'As of 2024 the country is the second biggest grain exporter in the world, with 19% of the international market share, and the fourth overall grain producer.', 'The impacts of tighter financial conditions will build over time, and pressures on fiscal balances will rise. Given historically high debt ...']\u001B[0m\u001B[32;1m\u001B[1;3mI have gathered relevant information regarding the current economic situation in Brazil. The economy has shown resilience in 2023, with strong wages and exports, but challenges such as restraining inflation and a weak currency could hinder further progress. Brazil's economic growth has averaged above 3% over the past three years, driven by robust domestic demand and private consumption. However, the country also faces significant public debt and policy uncertainty.\n",
      "\n",
      "Now, I will analyze the trends in orange exports based on the gathered data.\n",
      "\n",
      "1. **Weather Conditions**: There is a forecasted decline in orange production in Brazil due to adverse weather conditions, particularly in key growing regions. However, the industry shows resilience, and production forecasts remain relatively robust.\n",
      "\n",
      "2. **Global Demand**: The global orange market is projected to grow at a CAGR of 3.2% from 2024 to 2030, indicating a steady increase in demand for oranges. This growth is driven by trends towards organic and sustainably sourced products.\n",
      "\n",
      "3. **Economic Situation**: Brazil's economy is recovering post-COVID-19, with strong export performance. However, inflation and a weak currency pose challenges that could affect export volumes.\n",
      "\n",
      "**Trend Analysis**: \n",
      "- From 2021 to 2024, the export volumes of oranges have fluctuated: 200t in 2021, 190t in 2022, 210t in 2023, and a projected 220t in 2024. This indicates a slight upward trend in exports, despite the challenges faced.\n",
      "\n",
      "**Forecast for 2025**: \n",
      "Considering the current trends and data:\n",
      "- If the weather conditions stabilize and production improves, alongside the growing global demand, it is reasonable to estimate a continued increase in exports.\n",
      "- Given the current trajectory and the economic factors at play, I would estimate orange exports for 2025 to be around **230-240 tons**. This estimate accounts for potential recovery in production and sustained demand growth, despite the economic challenges.\n",
      "\n",
      "Final Answer: The estimated orange exports for 2025 are projected to be around 230-240 tons, considering the current weather conditions, global demand trends, and the economic situation in Brazil.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '\\nМи експортуємо апельсини з Бразилії. В 2021 — 200т, в 2022 — 190т, в 2023 — 210т, в 2024 (ще не завершено) — 220т.\\nЗроби оцінку на 2025 рік, враховуючи:\\n- погодні умови в Бразилії (використовуй openweathermap tool)\\n- попит на апельсини у світі (використовуй SerpAPI tool),\\n- економічну ситуацію (використовуй SerpAPI tool).\\n\\nВидай мені наступний результат:\\n1. Знайди останні новини про погодні умови в Бразилії.\\n2. Знайди статті про глобальний попит на апельсини.\\n3. Оціни тренд експорту.\\n4. Зроби прогноз на 2025 (в тоннах) з поясненням.\\n\\nЯкщо не можеш знайти інформацію, то напиши, що не можеш знайти дані і не пробуй прогнозувати. Треба лише чітка оцінка.\\n',\n",
       " 'output': 'The estimated orange exports for 2025 are projected to be around 230-240 tons, considering the current weather conditions, global demand trends, and the economic situation in Brazil.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Висновок\n",
    "\n",
    "> Final Answer: The estimated orange exports for 2025 are projected to be around 230-240 tons, considering the current weather conditions, global demand trends, and the economic situation in Brazil.\n",
    "\n",
    "Що із цього можна покращити - взагалі основне це покращити промпт, бо від нього багато чого залежить. Так наприклад openweathermap не був використав в процесі взагалі. Був використав пошук по погоді. Чомусь використовувся 2023 рік для аналізу, але тоді було б краще по кожному року із кожним експортом щоб зрозуміти що саме на то впливало. Також мабуть на одній все ж мові задавав опис tools та сам query - або все на українській, або на англійській, щоб більш точно було б."
   ]
  }
 ]
}
